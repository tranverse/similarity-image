import os
import shutil
import numpy as np
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import load_model
from tensorflow.keras.applications.efficientnet import preprocess_input
import sys
import io
import torch
from torchvision import models
import timm
from tensorflow.keras.preprocessing import image as keras_image
from torchvision import transforms
from PIL import Image

import torch.nn as nn
import urllib.parse

import timm
import torch.nn as nn
import torch
from torch.optim import AdamW
# Thi·∫øt l·∫≠p stdout ƒë·ªÉ in UTF-8 (ch·ªâ c·∫ßn tr√™n Windows)
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
model_path = r"E:\similarity_image\models\convnextv2\convnext_v2_best_params_aug2_final.pth"
input_dir = r"E:\LuanVan\data\test-others"
output_dir = r"E:\LuanVan\data\phanloai_ketqua1"
img_size = (224, 224)

def build_convnext_v2_aug_model(num_classes, lr=0.000188, dropout_rate=0.204782, dense_units=576,
                l2_reg=1.792511e-07 , fine_tune_all=False, unfreeze_from_stage=2):

    # T·∫£i m√¥ h√¨nh ConvNeXt v2 v·ªõi pretrained weights
    model = timm.create_model('convnextv2_tiny.fcmae_ft_in1k', pretrained=True, num_classes=0)
    in_features = model.head.in_features

    # ƒê√≥ng bƒÉng to√†n b·ªô n·∫øu kh√¥ng fine-tune t·∫•t c·∫£
    for param in model.parameters():
        param.requires_grad = False

    # M·ªü c√°c stage t·ª´ v·ªã tr√≠ ch·ªâ ƒë·ªãnh (unfreeze t·ª´ stage 2 tr·ªü ƒëi)
    for i, stage in enumerate(model.stages):
        if i >= unfreeze_from_stage:
            for param in stage.parameters():
                param.requires_grad = True

    # Head m·ªõi v·ªõi c√°c tham s·ªë t·ª´ trial
    model.head = nn.Sequential(
        nn.AdaptiveAvgPool2d(1),
        nn.Flatten(),
        nn.Linear(in_features, dense_units),
        nn.GELU(),  
        nn.Dropout(dropout_rate),
        nn.Linear(dense_units, num_classes)
    )

    return model


def load_convnextv2_model(model_path, num_classes):
    model = build_convnext_v2_aug_model(num_classes=num_classes)
    
    # Load tr·ªçng s·ªë
    state_dict = torch.load(model_path, map_location='cpu')

    # Tr∆∞·ªùng h·ª£p l∆∞u theo d·∫°ng full dict c√≥ 'model_state_dict'
    if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:
        model_state_dict = state_dict['model_state_dict']
    else:
        model_state_dict = state_dict  # Tr∆∞·ªùng h·ª£p load tr·ª±c ti·∫øp state_dict

    if 'head.5.weight' in model_state_dict:
        print("‚úî Found key: head.5.weight, Shape:", model_state_dict['head.5.weight'].shape)
    else:
        print("‚ö† Kh√¥ng t√¨m th·∫•y key 'head.5.weight'. C√≥ th·ªÉ model ƒë√£ ƒë·ªïi c·∫•u tr√∫c head.")

    # Load tr·ªçng s·ªë v√†o m√¥ h√¨nh
    missing_keys, unexpected_keys = model.load_state_dict(model_state_dict, strict=False)

    # Th√¥ng b√°o n·∫øu c√≥ keys kh√¥ng kh·ªõp
    if missing_keys:
        print("‚ö† Missing keys:", missing_keys)
    if unexpected_keys:
        print("‚ö† Unexpected keys:", unexpected_keys)

    if missing_keys or unexpected_keys:
        print("‚ö† C·∫£nh b√°o: C√≥ key kh√¥ng kh·ªõp, ki·ªÉm tra l·∫°i s·ªë l·ªõp ho·∫∑c c·∫•u tr√∫c head c·ªßa m√¥ h√¨nh!")

    model.eval()
    return model

preprocess_torch = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# Load model
model = load_convnextv2_model(model_path, 11)

# L·∫•y t√™n l·ªõp t·ª´ th∆∞ m·ª•c hu·∫•n luy·ªán c≈©
ref_dir = r"E:\LuanVan\data\split-raw\train"
class_names = sorted([d for d in os.listdir(ref_dir) if os.path.isdir(os.path.join(ref_dir, d))])
print("üîç L·ªõp ph√¢n lo·∫°i:", class_names)

# T·∫°o th∆∞ m·ª•c ƒë√≠ch cho t·ª´ng l·ªõp
for class_name in class_names:
    os.makedirs(os.path.join(output_dir, class_name), exist_ok=True)

# Duy·ªát qua c√°c ·∫£nh trong th∆∞ m·ª•c c·∫ßn ph√¢n lo·∫°i
for fname in os.listdir(input_dir):
    if fname.lower().endswith(('.png', '.jpg', '.jpeg')):
        img_path = os.path.join(input_dir, fname)

        img = Image.open(img_path).convert("RGB")
        img_tensor = preprocess_torch(img).unsqueeze(0).to(device)
        with torch.no_grad():
            preds = model(img_tensor)
            preds = torch.softmax(preds, dim=1)
        class_id = torch.argmax(preds[0]).item()
        confidence = preds[0][class_id].item()
        predicted_class = class_names[class_id]

        # Decode t√™n file t·ª´ URL encoding sang t√™n b√¨nh th∆∞·ªùng
        decoded_fname = urllib.parse.unquote(fname)

        # T·∫°o th∆∞ m·ª•c ƒë√≠ch n·∫øu ch∆∞a c√≥
        dest_dir = os.path.join(output_dir, predicted_class)
        os.makedirs(dest_dir, exist_ok=True)

        # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë√≠ch
        dest_path = os.path.join(dest_dir, decoded_fname)

        # Di chuy·ªÉn ·∫£nh v√†o th∆∞ m·ª•c l·ªõp t∆∞∆°ng ·ª©ng
        
        shutil.copy(img_path, dest_path)
        print(f"üìÇ {fname} ‚ûú {predicted_class}")

print("‚úÖ Ph√¢n lo·∫°i ho√†n t·∫•t!")
